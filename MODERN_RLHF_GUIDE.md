# üöÄ Modern RLHF Framework - –ü–æ–ª–Ω–æ–µ –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ

## üìã –û–±–∑–æ—Ä

–Ø —Å–æ–∑–¥–∞–ª –¥–ª—è –≤–∞—Å **—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é, —á–∏—Å—Ç—É—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é** —Å–∏—Å—Ç–µ–º—É RLHF –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã –≤–∞—à–µ–≥–æ —Å—Ç–∞—Ä–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞:

### ‚úÖ –ß—Ç–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ:
- **–ß–∏—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** - –º–æ–¥—É–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –±–µ–∑ –∑–∞–ø—É—Ç–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞
- **–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã** - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ PPO –∏ DPO (Direct Preference Optimization)
- **–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏** - BERTScore, CodeBLEU, BLEU, ROUGE, Ruby
- **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è human feedback** - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ human logits –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Å–ª–æ–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
- **GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU —Å mixed precision
- **–¶–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏** - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤–∞—à–∏—Ö —Ü–µ–ª–µ–π (>0.7 –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫)

## üèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ü—Ä–æ–µ–∫—Ç–∞

```
modern_rlhf/
‚îú‚îÄ‚îÄ __init__.py          # –û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å
‚îú‚îÄ‚îÄ config.py            # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
‚îú‚îÄ‚îÄ metrics.py           # –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏
‚îú‚îÄ‚îÄ reward_model.py      # Reward –º–æ–¥–µ–ª—å —Å human feedback
‚îú‚îÄ‚îÄ trainer.py           # PPO/DPO —Ç—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫–∏
‚îú‚îÄ‚îÄ pipeline.py          # –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è
‚îú‚îÄ‚îÄ data_loader.py       # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ main.py             # CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îú‚îÄ‚îÄ requirements.txt     # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îî‚îÄ‚îÄ README.md           # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
```

## üöÄ –ë—ã—Å—Ç—Ä—ã–π –°—Ç–∞—Ä—Ç

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
```bash
cd modern_rlhf
pip install -r requirements.txt
```

### 2. –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—É—Å–∫
```bash
# –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
python ../run_modern_rlhf.py

# –ò–ª–∏ —á–µ—Ä–µ–∑ CLI
python main.py --mode fast --epochs 2 --steps 500
```

### 3. –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
```bash
# –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ä–µ–∂–∏–º
python main.py --mode research --epochs 10 --steps 2000

# –ü—Ä–æ–¥–∞–∫—à–Ω —Ä–µ–∂–∏–º
python main.py --mode production --device cuda --batch-size 8
```

## üéØ –¶–µ–ª–µ–≤—ã–µ –ú–µ—Ç—Ä–∏–∫–∏

–°–∏—Å—Ç–µ–º–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤–∞—à–∏—Ö —Ü–µ–ª–µ–π:

| –ú–µ—Ç—Ä–∏–∫–∞ | –¶–µ–ª—å | –û–ø–∏—Å–∞–Ω–∏–µ |
|---------|------|----------|
| **BERTScore** | ‚â• 0.7 | –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ |
| **CodeBLEU** | ‚â• 0.6 | –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –¥–ª—è –∫–æ–¥–∞ –æ—Ü–µ–Ω–∫–∞ |
| **BLEU** | ‚â• 0.4 | N-gram overlap |
| **ROUGE** | ‚â• 0.5 | –ú–µ—Ç—Ä–∏–∫–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ |
| **Ruby** | ‚â• 0.3 | –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞ |

## üîß –ö–ª—é—á–µ–≤—ã–µ –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

### 1. Human Feedback Integration
```python
# –ü–æ–¥–¥–µ—Ä–∂–∫–∞ human logits –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Å–ª–æ–µ
config.reward.use_human_logits = True
config.reward.human_logits_layer = "last"  # –∏–ª–∏ "second_last"
config.reward.human_feedback_weight = 0.3
```

### 2. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ú–µ—Ç–æ–¥—ã –û–±—É—á–µ–Ω–∏—è
- **PPO** - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π RLHF
- **DPO** - Direct Preference Optimization (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥)
- **Mixed Precision** - –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
- **Gradient Checkpointing** - –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏

### 3. –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è Reward –ú–æ–¥–µ–ª—å
```python
# –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã reward –º–æ–¥–µ–ª–∏:
- Syntax correctness (20%)
- Execution success (30%) 
- Semantic similarity (30%)
- Human preferences (20%)
```

### 4. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –û—Ü–µ–Ω–∫–∞
- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ –∫–æ–¥–∞
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
- –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞
- –û—Ü–µ–Ω–∫–∞ —Å—Ç–∏–ª—è –∫–æ–¥–∞

## üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
```python
from modern_rlhf import ModernRLHFPipeline, get_research_config

# –°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
config = get_research_config()

# –°–æ–∑–¥–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω
pipeline = ModernRLHFPipeline(config)

# –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
results = pipeline.run_full_pipeline()

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
if results.success:
    print(f"BERTScore: {results.evaluation_metrics['bertscore']:.3f}")
    print(f"CodeBLEU: {results.evaluation_metrics['codebleu']:.3f}")
```

### –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
```python
from modern_rlhf.config import ModernRLHFConfig

config = ModernRLHFConfig()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏
config.model.base_model_name = "microsoft/CodeGPT-small-py"
config.model.reward_model_name = "microsoft/codebert-base"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
config.training.learning_rate = 1e-5
config.training.batch_size = 4
config.training.ppo_epochs = 10

# –¶–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
config.evaluation.target_bertscore = 0.8
config.evaluation.target_codebleu = 0.7

# Human feedback
config.reward.use_human_logits = True
config.reward.human_feedback_weight = 0.3
```

## üéõÔ∏è CLI –ö–æ–º–∞–Ω–¥—ã

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
```bash
# –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–æ—Ç–æ—Ç–∏–ø
python main.py --mode fast

# –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
python main.py --mode research --epochs 10

# –ü—Ä–æ–¥–∞–∫—à–Ω –æ–±—É—á–µ–Ω–∏–µ
python main.py --mode production --device cuda

# –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
python main.py \
    --learning-rate 1e-5 \
    --batch-size 4 \
    --model-name microsoft/CodeGPT-small-py \
    --target-bertscore 0.8 \
    --target-codebleu 0.7
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
- `--mode`: research/production/fast
- `--learning-rate`: —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
- `--batch-size`: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
- `--epochs`: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
- `--device`: cpu/cuda/auto
- `--target-*`: —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è

–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç:
- –ì—Ä–∞—Ñ–∏–∫–∏ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π
- –î–µ—Ç–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤

## üî¨ –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### 1. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å DPO
```python
# –í–∫–ª—é—á–∏—Ç—å DPO –≤–º–µ—Å—Ç–æ PPO
config.training.use_dpo = True
config.training.dpo_beta = 0.1
config.training.dpo_loss_type = "sigmoid"
```

### 2. –ê–±–ª—è—Ü–∏–æ–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
```python
# –û—Ç–∫–ª—é—á–∏—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã reward –º–æ–¥–µ–ª–∏
config.reward.syntax_reward_weight = 0.0
config.reward.execution_reward_weight = 0.0
config.reward.semantic_reward_weight = 1.0
```

### 3. –†–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏
```python
# –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
config.model.base_model_name = "microsoft/CodeGPT-small-py"
# –∏–ª–∏
config.model.base_model_name = "Salesforce/codegen-350M-mono"
```

## üöÄ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ù–æ–≤–æ–π –°–∏—Å—Ç–µ–º—ã

### –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞—Ä—ã–º –ø—Ä–æ–µ–∫—Ç–æ–º:

1. **–ß–∏—Å—Ç–æ—Ç–∞ –∫–æ–¥–∞**: –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –ª–µ–≥–∫–æ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å
2. **–°–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–µ—Ç–æ–¥—ã (DPO, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏)
3. **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å**: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è GPU, mixed precision
4. **–ì–∏–±–∫–æ—Å—Ç—å**: –õ–µ–≥–∫–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏
5. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
6. **–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫, –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö

## üìù –°–ª–µ–¥—É—é—â–∏–µ –®–∞–≥–∏

1. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**:
   ```bash
   pip install -r modern_rlhf/requirements.txt
   ```

2. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç**:
   ```bash
   python test_modern_rlhf.py
   ```

3. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ –±—ã—Å—Ç—Ä—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç**:
   ```bash
   python run_modern_rlhf.py
   ```

4. **–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ–¥ –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ**:
   - –û–±–Ω–æ–≤–∏—Ç–µ –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
   - –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
   - –í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å

5. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**:
   ```bash
   python modern_rlhf/main.py --mode research --epochs 10
   ```

## üéâ –†–µ–∑—É–ª—å—Ç–∞—Ç

–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å:
- ‚úÖ **–ß–∏—Å—Ç–∞—è, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ RLHF**
- ‚úÖ **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ç—Ä–∏–∫**
- ‚úÖ **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è human feedback**
- ‚úÖ **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU**
- ‚úÖ **–ì–∏–±–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è**
- ‚úÖ **–ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π —Ä–∞–±–æ—Ç–µ**

–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤–∞—à–∏—Ö —Ü–µ–ª–µ–π –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ RLHF –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞!
