#!/usr/bin/env python3
"""
Quick Start Script for Modern RLHF
==================================

Simple script to run the modern RLHF framework with your existing data.
"""

import sys
import os
from pathlib import Path

# Add modern_rlhf to path
sys.path.insert(0, str(Path(__file__).parent / "modern_rlhf"))

from modern_rlhf import ModernRLHFPipeline, get_research_config
from modern_rlhf.config import ModernRLHFConfig

def main():
    """Quick start function."""
    print("ğŸš€ Modern RLHF Framework - Quick Start")
    print("=" * 50)
    
    # Create configuration
    config = get_research_config()
    
    # Adjust paths to use existing data
    config.data.train_data_path = r"C:\Users\ĞŸĞ¾Ğ»Ğ¸Ğ½Ğ°\Desktop\Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°\huawei\rlhf\conala-corpus\conala-train.json"
    config.data.eval_data_path = r"C:\Users\ĞŸĞ¾Ğ»Ğ¸Ğ½Ğ°\Desktop\Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°\huawei\rlhf\conala-corpus\conala-test.json"
    config.data.human_feedback_path = "./evaluation_results_server"
    config.data.output_path = "./modern_outputs"
    config.data.min_prompt_length = 0
    config.data.min_response_length = 0
    # Force local CoNaLa corpus (preferred over Hub)
    config.data.conala_local_path = r"C:\Users\ĞŸĞ¾Ğ»Ğ¸Ğ½Ğ°\Desktop\Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°\huawei\rlhf\conala-corpus"
    
    # Set experiment name
    config.experiment_name = "modern_rlhf_experiment"
    
    # Adjust training parameters for better convergence
    config.training.ppo_epochs = 10
    config.training.total_steps = 2000
    config.evaluation.eval_samples = 100
    config.training.learning_rate = 1e-5
    
    # Set target metrics
    config.evaluation.target_bertscore = 0.7
    config.evaluation.target_codebleu = 0.6
    config.evaluation.target_bleu = 0.4
    config.evaluation.target_rouge = 0.5
    config.evaluation.target_ruby = 0.3
    config.data.conala_local_path = r"C:\Users\ĞŸĞ¾Ğ»Ğ¸Ğ½Ğ°\Desktop\Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°\huawei\rlhf\conala-corpus"
    
    print(f"ğŸ“ Training data: {config.data.train_data_path}")
    print(f"ğŸ“ Evaluation data: {config.data.eval_data_path}")
    print(f"ğŸ“ Human feedback: {config.data.human_feedback_path}")
    print(f"ğŸ“ Output directory: {config.data.output_path}")
    if getattr(config.data, 'conala_local_path', None):
        print(f"ğŸ“ CoNaLa local corpus: {config.data.conala_local_path}")
    print(f"ğŸ¯ Target BERTScore: {config.evaluation.target_bertscore}")
    print(f"ğŸ¯ Target CodeBLEU: {config.evaluation.target_codebleu}")
    print(f"ğŸ¯ Target BLEU: {config.evaluation.target_bleu}")
    print(f"ğŸ¯ Target ROUGE: {config.evaluation.target_rouge}")
    print(f"ğŸ¯ Target Ruby: {config.evaluation.target_ruby}")
    print()
    
    # Create output directory
    os.makedirs(config.data.output_path, exist_ok=True)
    
    try:
        # Create pipeline
        print("ğŸ”§ Initializing Modern RLHF Pipeline...")
        pipeline = ModernRLHFPipeline(config)
        
        # Run pipeline
        print("ğŸƒ Starting training pipeline...")
        results = pipeline.run_full_pipeline()
        
        # Create visualizations
        print("ğŸ“Š Creating visualizations...")
        pipeline.visualize_results()
        
        # Print results
        print("\n" + "=" * 50)
        print("ğŸ“ˆ RESULTS")
        print("=" * 50)
        
        if results.success:
            print("âœ… Pipeline completed successfully!")
            print(f"â±ï¸  Total time: {results.total_time:.2f} seconds")
            print(f"â±ï¸  Training time: {results.training_time:.2f} seconds")
            
            print("\nğŸ“Š Final Metrics:")
            for metric, value in results.final_metrics.items():
                print(f"  {metric}: {value}")
            
            print("\nğŸ“Š Evaluation Metrics:")
            for metric, value in results.evaluation_metrics.items():
                if isinstance(value, (int, float)):
                    print(f"  {metric}: {value:.4f}")
            
            # Check targets
            if 'targets_met' in results.evaluation_metrics:
                targets_met = results.evaluation_metrics['targets_met']
                met_count = sum(targets_met.values())
                total_count = len(targets_met)
                print(f"\nğŸ¯ Targets Met: {met_count}/{total_count}")
                
                if met_count == total_count:
                    print("ğŸ‰ All targets achieved!")
                else:
                    print("âš ï¸  Some targets not met:")
                    for metric, met in targets_met.items():
                        status = "âœ…" if met else "âŒ"
                        print(f"  {status} {metric}")
            
            print(f"\nğŸ“ Results saved to: {config.data.output_path}")
            
        else:
            print("âŒ Pipeline failed!")
            print(f"Error: {results.error_message}")
            
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
