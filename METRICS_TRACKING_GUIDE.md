# üìä Metrics Tracking Guide

## –û–±–∑–æ—Ä

–°–∏—Å—Ç–µ–º–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ª–æ–≥–∏—Ä—É–µ—Ç –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è RLHF –º–æ–¥–µ–ª–∏. –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è:

- **CSV —Ñ–∞–π–ª—ã** —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
- **JSON —Ñ–∞–π–ª—ã** —Å –ø–æ–ª–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π
- **–ì—Ä–∞—Ñ–∏–∫–∏ PNG** –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
- **–ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥** —Å summary –ø–æ —ç–ø–æ—Ö–µ

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–µ—Ç—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:

```
modern_outputs/
‚îî‚îÄ‚îÄ metrics/
    ‚îú‚îÄ‚îÄ training_metrics.csv          # CSV —Å–æ –≤—Å–µ–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    ‚îú‚îÄ‚îÄ training_metrics.json         # JSON —Å –ø–æ–ª–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π
    ‚îî‚îÄ‚îÄ plots/
        ‚îú‚îÄ‚îÄ training_progress.png     # Loss –∏ Reward
        ‚îú‚îÄ‚îÄ policy_metrics.png        # KL Divergence –∏ Entropy
        ‚îî‚îÄ‚îÄ evaluation_metrics.png    # BERTScore, CodeBLEU, etc.
```

---

## üìà –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏

### Training Metrics (–∫–∞–∂–¥–∞—è —ç–ø–æ—Ö–∞)

| –ú–µ—Ç—Ä–∏–∫–∞ | –û–ø–∏—Å–∞–Ω–∏–µ |
|---------|----------|
| `loss` | Training loss (Policy + Value + Entropy) |
| `reward` | –°—Ä–µ–¥–Ω–∏–π reward –æ—Ç reward model |
| `kl_divergence` | KL divergence –º–µ–∂–¥—É policy –∏ reference model |
| `entropy` | –≠–Ω—Ç—Ä–æ–ø–∏—è policy (exploration measure) |
| `learning_rate` | –¢–µ–∫—É—â–∏–π learning rate |
| `epoch_time` | –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç–ø–æ—Ö–∏ (—Å–µ–∫—É–Ω–¥—ã) |
| `samples_per_second` | –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ samples |

### Evaluation Metrics (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ evaluation)

| –ú–µ—Ç—Ä–∏–∫–∞ | –û–ø–∏—Å–∞–Ω–∏–µ |
|---------|----------|
| `bertscore` | Semantic similarity (BERTScore) |
| `codebleu` | Code-specific BLEU score |
| `bleu` | Standard BLEU score |
| `rouge` | ROUGE-L score |
| `ruby` | Code quality metric |

---

## üéØ –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã

### –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —ç–ø–æ—Ö–∏

–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤–Ω—É—Ç—Ä–∏ —ç–ø–æ—Ö–∏ —Å live –º–µ—Ç—Ä–∏–∫–∞–º–∏:

```
Epoch 3/10 |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [05:32<00:00, 3.32s/batch]
  loss: 2.3456  reward: 0.7821  lr: 1.00e-05  step: 300/1000
```

**–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç:**
- –¢–µ–∫—É—â–∏–π batch / –í—Å–µ–≥–æ batches
- –ü—Ä–æ—à–µ–¥—à–µ–µ –≤—Ä–µ–º—è / –û—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è
- –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ (batches/sec)
- –¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (loss, reward, lr, step)

### –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä Evaluation

–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏:

```
Evaluating |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [02:15<00:00, 6.75s/batch]
  processed: 160  avg_reward: 0.7234
```

---

## üìä Summary –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç–ø–æ—Ö–∏ –ø–µ—á–∞—Ç–∞–µ—Ç—Å—è detailed summary:

```
================================================================================
EPOCH 3 SUMMARY
================================================================================

[Training Metrics]
  Loss:          2.345678
  Reward:        0.782145
  KL Divergence: 0.012345
  Entropy:       2.567890
  Learning Rate: 1.00e-05

[Evaluation Metrics]
  BERTScore:  0.5234
  CodeBLEU:   0.3891
  BLEU:       0.2678
  ROUGE:      0.3456
  RUBY:       0.2134

[Performance]
  Epoch Time:             332.50s
  Samples/sec:            4.82
  Estimated Time Remaining: 38.9 min (2334s)

================================================================================
```

---

## üìà –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞—Ñ–∏–∫–∏

### 1. Training Progress (`training_progress.png`)

–î–≤–∞ –≥—Ä–∞—Ñ–∏–∫–∞:
- **Loss over Epochs**: –ö–∞–∫ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è loss
- **Reward over Epochs**: –ö–∞–∫ —Ä–∞—Å—Ç–µ—Ç reward

### 2. Policy Metrics (`policy_metrics.png`)

–î–≤–∞ –≥—Ä–∞—Ñ–∏–∫–∞:
- **KL Divergence**: –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ —Å reference model
- **Entropy**: Exploration/exploitation balance

### 3. Evaluation Metrics (`evaluation_metrics.png`)

–í—Å–µ evaluation –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ–¥–Ω–æ–º –≥—Ä–∞—Ñ–∏–∫–µ:
- BERTScore (–∫—Ä–∞—Å–Ω—ã–π)
- CodeBLEU (—Å–∏–Ω–∏–π)
- BLEU (–∑–µ–ª–µ–Ω—ã–π)
- ROUGE (–æ—Ä–∞–Ω–∂–µ–≤—ã–π)
- RUBY (—Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π)

---

## üíª –ü—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –º–µ—Ç—Ä–∏–∫–∞–º

### –ß—Ç–µ–Ω–∏–µ CSV

```python
import pandas as pd

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
df = pd.read_csv('modern_outputs/metrics/training_metrics.csv')

# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —ç–ø–æ—Ö
print(df.tail())

# –ù–∞–π—Ç–∏ –ª—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
best_reward_epoch = df.loc[df['reward'].idxmax()]
print(f"Best reward: {best_reward_epoch['reward']} at epoch {best_reward_epoch['epoch']}")
```

### –ß—Ç–µ–Ω–∏–µ JSON

```python
import json

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
with open('modern_outputs/metrics/training_metrics.json', 'r') as f:
    history = json.load(f)

# –ü–æ—Å–ª–µ–¥–Ω—è—è —ç–ø–æ—Ö–∞
last_epoch = history[-1]
print(f"Epoch {last_epoch['epoch']}: Loss={last_epoch['loss']}, Reward={last_epoch['reward']}")

# –í—Å–µ rewards
rewards = [epoch['reward'] for epoch in history]
print(f"Average reward: {sum(rewards)/len(rewards)}")
```

### –ß–µ—Ä–µ–∑ MetricsTracker API

```python
from modern_rlhf.metrics_tracker import MetricsTracker

# –°–æ–∑–¥–∞—Ç—å tracker
tracker = MetricsTracker(output_dir="./modern_outputs/metrics")

# –ü–æ–ª—É—á–∏—Ç—å –ª—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
best = tracker.get_best_metrics()
print(f"Best reward: {best['best_reward']}")
print(f"Lowest loss: {best['lowest_loss']}")

# –ü–æ–ª—É—á–∏—Ç—å summary
summary = tracker.export_summary()
print(f"Total epochs: {summary['total_epochs']}")
print(f"Total training time: {summary['total_training_time']:.2f}s")
```

---

## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞

### –û—Ç–∫–ª—é—á–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ (–µ—Å–ª–∏ –Ω–µ—Ç matplotlib)

–ì—Ä–∞—Ñ–∏–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª—é—á–∞—é—Ç—Å—è, –µ—Å–ª–∏ matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω:

```bash
# –£–¥–∞–ª–∏—Ç—å matplotlib –∏–∑ requirements.txt
# –ò–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å: pip install torch transformers trl
```

### –ò–∑–º–µ–Ω–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤—ã–≤–æ–¥–∞

–í `config.py`:

```python
@dataclass
class DataConfig:
    output_path: str = "./my_custom_output"  # –ò–∑–º–µ–Ω–∏—Ç—å –∑–¥–µ—Å—å
```

–ú–µ—Ç—Ä–∏–∫–∏ –±—É–¥—É—Ç –≤ `./my_custom_output/metrics/`

### –ò–∑–º–µ–Ω–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏. –≠—Ç–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ `MetricsTracker`:

```python
# –í modern_rlhf/metrics_tracker.py
class MetricsTracker:
    def __init__(self, output_dir: str = "./modern_outputs/metrics", save_every_n_epochs: int = 1):
        self.save_every_n_epochs = save_every_n_epochs
```

---

## üìã –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
python fix_training.py

# –í –¥—Ä—É–≥–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ - —Å–º–æ—Ç—Ä–µ—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ real-time
watch -n 5 tail -n 20 modern_outputs/metrics/training_metrics.csv
```

### –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è

```python
import pandas as pd
import matplotlib.pyplot as plt

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
df = pd.read_csv('modern_outputs/metrics/training_metrics.csv')

# –ü–æ—Å—Ç—Ä–æ–∏—Ç—å custom –≥—Ä–∞—Ñ–∏–∫
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(df['epoch'], df['reward'], marker='o', label='Reward')
ax.plot(df['epoch'], df['loss'], marker='s', label='Loss')
ax.set_xlabel('Epoch')
ax.set_ylabel('Value')
ax.legend()
ax.grid(True)
plt.savefig('my_custom_plot.png')
plt.show()
```

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤

```python
import pandas as pd

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö runs
run1 = pd.read_csv('run1/metrics/training_metrics.csv')
run2 = pd.read_csv('run2/metrics/training_metrics.csv')

# –°—Ä–∞–≤–Ω–∏—Ç—å
print(f"Run 1 best reward: {run1['reward'].max()}")
print(f"Run 2 best reward: {run2['reward'].max()}")

# –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
import matplotlib.pyplot as plt
plt.plot(run1['epoch'], run1['reward'], label='Run 1')
plt.plot(run2['epoch'], run2['reward'], label='Run 2')
plt.legend()
plt.savefig('comparison.png')
```

---

## üöÄ Best Practices

### 1. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ Real-time

–û—Ç–∫—Ä—ã–≤–∞–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏ –≤ background –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:

```bash
# Linux/Mac
watch -n 30 eog modern_outputs/metrics/plots/training_progress.png

# Windows
# –û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª –∏ –≤–∫–ª—é—á–∏—Ç–µ auto-refresh –≤ –≤–∞—à–µ–º image viewer
```

### 2. –†–µ–≥—É–ª—è—Ä–Ω—ã–µ Checkpoints

```python
# –í config.py
save_steps: int = 100  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å checkpoint –∫–∞–∂–¥—ã–µ 100 steps
```

### 3. Early Stopping

–°–ª–µ–¥–∏—Ç–µ –∑–∞ `reward` –∏ `loss`:
- –ï—Å–ª–∏ `reward` –Ω–µ —Ä–∞—Å—Ç–µ—Ç 5+ —ç–ø–æ—Ö ‚Üí –≤–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
- –ï—Å–ª–∏ `loss` –Ω–µ –ø–∞–¥–∞–µ—Ç ‚Üí –≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ —Å LR

### 4. Backup Metrics

```bash
# –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–π—Ç–µ metrics –≤ backup
cp -r modern_outputs/metrics backups/metrics_$(date +%Y%m%d_%H%M%S)
```

---

## üêõ Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: –ì—Ä–∞—Ñ–∏–∫–∏ –Ω–µ —Å–æ–∑–¥–∞—é—Ç—Å—è

**–†–µ—à–µ–Ω–∏–µ**: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ matplotlib:
```bash
pip install matplotlib seaborn
```

### –ü—Ä–æ–±–ª–µ–º–∞: CSV —Ñ–∞–π–ª –ø—É—Å—Ç–æ–π

**–†–µ—à–µ–Ω–∏–µ**: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—Å—Ç–∏–ª–æ—Å—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. –ú–µ—Ç—Ä–∏–∫–∏ –ª–æ–≥–∏—Ä—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–µ—Ä–≤–æ–π —ç–ø–æ—Ö–∏.

### –ü—Ä–æ–±–ª–µ–º–∞: –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è

**–†–µ—à–µ–Ω–∏–µ** (Windows):
```python
# –í modern_rlhf/trainer.py —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ:
tqdm_kwargs = {'ascii': True, 'ncols': 100}  # ASCII —Ä–µ–∂–∏–º –¥–ª—è Windows
```

### –ü—Ä–æ–±–ª–µ–º–∞: Metrics —Ñ–∞–π–ª—ã —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ

**–†–µ—à–µ–Ω–∏–µ**: –õ–æ–≥–∏—Ä—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ —Ä–µ–∂–µ –∏–ª–∏ –æ—á–∏—â–∞–π—Ç–µ —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã:
```bash
# –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–æ—Ö
tail -n 11 modern_outputs/metrics/training_metrics.csv > temp.csv
mv temp.csv modern_outputs/metrics/training_metrics.csv
```

---

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- **Metrics API**: `modern_rlhf/metrics_tracker.py`
- **Trainer Integration**: `modern_rlhf/trainer.py` (lines 103-106, 814-830, 1247-1261)
- **Config**: `modern_rlhf/config.py`

---

**‚úÖ –ì–æ—Ç–æ–≤–æ!** –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. –ü—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ, –∏ –º–µ—Ç—Ä–∏–∫–∏ –±—É–¥—É—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ real-time! üöÄ

