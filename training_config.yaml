# Training configuration for RLHF pipeline (SFT -> Reward -> PPO)
# Adjust paths, devices and hyperparameters to your hardware

env:
  python: "python"
  device: "cuda"   # or "cpu"
  output_dir: "outputs"
  work_dir: "."

paths:
  datasets_dir: "datasets_for_eval"
  prefs_folder: "evaluation_results_server"
  training_data_dir: "training_data"
  reward_model_out: "evaluation_results/reward_model"
  sft_model_out: "outputs/sft_model"
  ppo_model_out: "outputs/ppo_model"

reward_training:
  pairwise_epochs: 8
  batch_size: 32
  lr: 2e-5
  max_length: 512
  device: "cuda"
  early_stop_rounds: 3

sft:
  epochs: 3
  batch_size: 16
  lr: 1e-5
  max_length: 512
  gradient_accumulation_steps: 2

ppo:
  ppo_steps: 20000           # total PPO steps (tune up for better results)
  rollout_batch_size: 64     # prompts per rollout
  ppo_update_epochs: 4
  ppo_mini_batch_size: 64
  clip_epsilon: 0.2
  kl_coef: 0.02
  value_lr: 5e-5
  policy_lr: 1e-5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  gamma: 1.0
  lam: 0.95

evaluation:
  sample_size: 200
  decode:
    do_sample: false
    temperature: 1.0
    top_p: 0.95
    num_beams: 5
    max_gen_length: 256

logging:
  log_interval_steps: 100
  eval_interval_steps: 2000
  save_interval_steps: 2000

# Example commands (run from repo root):
# 1) create environment and install deps
#    conda create -n rlhfenv python=3.11 -y
#    conda activate rlhfenv
#    pip install -r requirements-rlhf.txt
#    # if CUDA wheel required, install torch manually per https://pytorch.org/get-started/locally/

# 2) prepare pairwise prefs and datasets
#    python scripts/prepare_pairs.py --prefs-folder evaluation_results_server --out training_data/

# 3) train reward model (pairwise)
#    python scripts/pref_convert_and_reward_train.py --prefs-folder evaluation_results_server --pairwise --pairwise-epochs 8 --device cuda

# 4) supervised finetune (SFT)
#    python scripts/finetune_supervised.py --data training_data/sft_train.jsonl --output_dir outputs/sft_model --epochs 3 --device cuda

# 5) RLHF / PPO
#    python scripts/main_rlhf_training.py --ppo-steps 20000 --device cuda --policy-model outputs/sft_model --reward-model evaluation_results/reward_model --output-dir outputs/ppo_model --kl-coef 0.02

# 6) evaluate
#    python scripts/evaluate_model.py --sample-size 200
